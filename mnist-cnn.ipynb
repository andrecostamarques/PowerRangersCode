{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "16734994",
      "metadata": {
        "id": "16734994"
      },
      "source": [
        "# MNIST With ConvNets\n",
        "In this notebook, we continue the work of \"mnist.ipynb\" of understanding Pytorch basics with MNIST. Our aim is to use convnets intead of a simple MLP, so only `Defining a Neural Network` and `Loss Function and Optmizer` sections will be new.\n",
        "\n",
        "**We will not focus on architecture choices, but on framework implementation and concepts understanding**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eecd4443",
      "metadata": {
        "id": "eecd4443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Imports\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Defines transform pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0,), (1,))\n",
        "])\n",
        "\n",
        "# Download and transform\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e700a67e",
      "metadata": {
        "id": "e700a67e"
      },
      "source": [
        "## Defining a Neural Network\n",
        "A key difference from our previous notebook is that a ConvNet has **convolutional layers**. These layers perform a crucial task called **feature extraction**â€”the process of capturing various abstract characteristics of an image by \"scanning\" different small sections of it.\n",
        "\n",
        "Our model will have 2 major parts:\n",
        "1. **Convolution**: for feature extraction;\n",
        "2. **Learning**: for classification/decision making;\n",
        "\n",
        "### 1. Convolution\n",
        "Defined with `nn.Conv2d`. In our model, we define 3 convolution layers with different sizes resulting in 1 channel of input (our images are grayscale) and 128 channels of output. At `foward()`, we define that at each layer will be a transformation like **convolution -> ReLU -> pooling**.\n",
        "- **Convolution:** feature extraction with\n",
        "    - **1 to 32 channels + ReLU + Pooling:** high resolution features with non-linearity;\n",
        "    - **32 to 64 channels + ReLU + Pooling:** less resolution features with non-linearity (more specific features);\n",
        "    - **53 to 128 channels + ReLU + Pooling:** low resolution feaures with non-linearity (general features);\n",
        "    - `kernel_size=3`: filter 3x3;\n",
        "    - `padding=\"same\"`: keeps image size;\n",
        "    - **dropout(25%):** deactivates 25% of the neurons to prevent overfitting;\n",
        "\n",
        "- **Learning:**\n",
        "    - **first fully connected layer:** (1152 -> 512) + ReLU\n",
        "    - **dropout(50%):** deactivates 50% of the neurons to prevent overfitting;\n",
        "    - **second fully connected layer:** (512 -> 10) + ReLU;\n",
        "    - **softmax (log):** last layer are classification probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "14418f32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14418f32",
        "outputId": "3aadee2d-c554-4e83-bfd7-6cf7aee92e85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SimpleCNN(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (dropout1): Dropout(p=0.25, inplace=False)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=1152, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=\"same\")\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=\"same\")\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=\"same\")\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # initiates with size = (1, 28, 28)\n",
        "        x = self.pool(F.relu(self.conv1(x))) # (32, 14, 14)\n",
        "        x = self.pool(F.relu(self.conv2(x))) # (64, 7, 7)\n",
        "        x = self.pool(F.relu(self.conv3(x))) # (128, 3, 3)\n",
        "        x = self.dropout1(x)\n",
        "        x = x.view(-1, 128 * 3 * 3)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a312719b",
      "metadata": {
        "id": "a312719b"
      },
      "source": [
        "## Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "debcefd8",
      "metadata": {
        "id": "debcefd8"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.NLLLoss()  # Negative Log-Likelihood Loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76151672",
      "metadata": {
        "id": "76151672"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7988a5ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7988a5ae",
        "outputId": "d4e7d4b7-e6d3-455b-b0f4-edad6cf1d139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.4805759787579367\n",
            "Epoch 2, Loss: 0.09399264111721725\n",
            "Epoch 3, Loss: 0.06570091240727571\n",
            "Epoch 4, Loss: 0.0527131122822783\n",
            "Epoch 5, Loss: 0.04246807015445858\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8f9c6bb2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f9c6bb2",
        "outputId": "1baa9d12-cdd1-44cf-af95-e5a8a87a81aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 98.70%\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on test set: {accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PowerRanger-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
